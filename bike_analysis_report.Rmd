---
title: "Forecasting Shared Bike Usage in London"
output:
  html_notebook:
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Import required libraries
#if(!exists("dat")) {source("bike_analysis.R")}
```


## Introduction

In this paper, I use data science and machine learning techniques learned on the HarvardX [*Data Science Professional Certificate*](https://www.edx.org/professional-certificate/harvardx-data-science) programme on [edX](https://www.edx.org/) to predict the demand for shared bikes in London. This paper forms my final project for the capstone course.

Kaggle, the online data science community, offers a competition to [forecast shared bike demand]( https://www.kaggle.com/c/bike-sharing-demand) on a scheme in Washington DC, the [Capital Bikeshare]( https://www.capitalbikeshare.com/about). Rather than taking that widely used dataset, I use data on a similar bike scheme in London, also available from the Kaggle platform. On the one hand, I examine shared bike demand because there are already a large number of analyses out there to learn from. On the other hand, I choose to examine London as opposed to Washington DC, since there are fewer analyses of London’s data already out there.

The demand for shared bikes is measured as a frequency or count variable, which tallies the number of new trips started each hour. As a result, it cannot take on negative values, and unlike the Gaussian distribution is skewed with a long tail towards higher count values.
Kaggle implicitly acknowledges the skewed shape of the distribution, since for any competition entry, before calculating the standard Root Mean Squared Error or RMSE measure of prediction performance, it first transforms the count outcome variable. Kaggle transforms both the actual count and its predicted counterpart by taking their logarithms, leading to what is known as the Root Mean Squared *Logarithmic* Error or RMSLE. I define the RMSLE in the [Methodology] section of this report.

Log-transforming the outcome variable is a common practice in data science, especially with count variables. By making this log-transformation, we assume that the count variable follows a distribution from the Poisson family of distributions.

In this paper, I compare a number of machine learning models for predicting bike demand. First, I train a standard linear model to predict the log-transformed count outcome. Second, I train two penalized linear models - the Ridge and the Lasso model - to see whether I can improve the prediction. As I explain in the [Methodology], the intuition behind these two models takes into account the bias-variance tradeoff: they reduce variance but at the expense of greater bias.

I compare the results of the various models using the RMSLE as the measure of goodness-of-fit, where the error is calculated by taking the difference between the log-transformation of the actual count variable and its predicted counterpart.

The remainder of this paper proceeds as follows:

1.	First, the [Exploring the Dataset] section describes and explores the London bike sharing dataset in detail. The results of this exploration feed directly into the choice of predictors in the modelling.

2.	Next, the [Methodology] section builds on the previous data exploration step by describing the modelling approach. It begins by outlining the objective of this paper and defines the RMSLE loss functions. It briefly describes the cleaning and feature preparation steps taken, before describing the particular machine learning models I apply.

3.	Next, the [Results] section describes the results of the various modelling approaches. It also presents a final RMSLE for prediction on a previously *untouched* dataset.

4.	Finally, the [Concluding Remarks] section offers an overall assessment of the work in this paper and areas for future work.


## Exploring the Dataset

The analysis in this paper uses a time series dataset downloaded from [Kaggle](https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset), which counts the number of new trips on London’s cycle hire scheme each hour for the period from `r format(summary_stats_full$start_date, "%d %B %Y")` to `r format(summary_stats_full$last_date, "%d %B %Y")`.

The bike scheme in London is officially called [Santander Cycles]( https://tfl.gov.uk/modes/cycling/santander-cycles) (previously Barclays Cycle Hire), though the bikes are colloquially known as “Boris bikes”, named after Boris Johnson, London’s mayor when the scheme was implemented. The scheme allows any member of the public to easily hire and ride a bike for a one-direction trip by collecting a bike from a [docking station](https://tfl.gov.uk/modes/cycling/santander-cycles/find-a-docking-station?intcmp=2321) and returning it to another station located around central London. As of today, there is a network of 750 docking stations around the UK’s capital with over 11,500 bikes.

Besides counting bike trips, the dataset also includes a number of other variables which are likely useful for predicting the number of trips, including the dates of bank holiday and weather data.

Although I download the full dataset from Kaggle, the dataset’s [webpage]( https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset) makes clear the original sources of the data. The bike data is originally gathered from [Transport for London (TfL)](https://cycling.data.tfl.gov.uk/), London’s transport authority, the dates of bank holidays from the [UK Government website]( https://www.gov.uk/bank-holidays), and the weather data from (freemeteo.com).


### Descriptive Statistics

Overall, the dataset contains \(n=\) `r format(summary_stats_full$total_obs, big.mark = ",")` observations, each representing a one hour period between `r format(summary_stats_full$start_date, "%d %B %Y")` and `r format(summary_stats_full$last_date, "%d %B %Y")`.

However, for the benefit of assessing the performance of the final algorithm, I partition the original dataset into two parts: a `training` dataset representing the first 90% of observations, ordered by `timestamp`, and a `validation` dataset representing the remaining 10%. The choice of 10% is fairly arbitrary, but the decision to set aside the final time-ordered observations is designed to simplify the calculation.

The exploratory analysis in this paper exclusively examines the `training` dataset, and the paper only turns to the `validation` set for calculating an out-of-sample measure of predictive performance using the final trained algorithm. The `training` set contains \(n=\) `r format(summary_stats_train$total_obs, big.mark = ",")` observations, each representing a one hour period between `r format(summary_stats_train$start_date, "%d %B %Y")` and `r format(summary_stats_train$last_date, "%d %B %Y")`. It spans `r format(summary_stats_train$total_days, big.mark = ",")` different days, counting `r format(summary_stats_train$total_trips, big.mark = ",")` bike journeys.

The table below describes the ten different columns included in the dataset. Besides time, date, season and journey data, the dataset includes a number of columns containing meteorological data.

```{r echo = FALSE}
data.frame(
  Name = c("Timestamp", "Journey Count", "T1", "T2", "Humidity", "Wind Speed", "Weather Code", "Is Holiday", "Is Weekend", "Season"),
  Variable = c("`timestamp`", "`cnt`", "`t1`", "`t2`", "`hum`", "`wind_speed`", "`weather_code`", "`is_holiday`", "`is_weekend`", "`season`"),
  Description = c("Date-time of observation, hourly",
                  "Count of new bike shares started in an hour",
                  "Real temperature in ̊C",
                  "'Feels like' temperature in ̊C",
                  "Humidity as a percentage",
                  "Wind speed in km per hour",
                  "1 = Clear / mostly clear but have some values with haze / Fog / Patches of fog / Fog in vicinity; 2 = Scattered clouds / Few clouds; 3 = Broken clouds; 4 = Cloudy; 7 = Rain / Light rain shower/ Light rain; 10 = Rain with thunderstorm; 26 = Snowfall; 94 = Freezing Fog",
                  "Boolean field - 1 holiday, 0 non-holiday",
                  "Boolean field - 1 weekend, 0 non-weekend",
                  "Category field, meteorological season - 0 spring, 1 summer, 2 autumn, 3 winter")) %>%
  knitr::kable( caption = "Variable Descriptions")
```

The table below reports standard summary statistics for each variable in the `training` dataset for which it makes sense, besides the `timestamp`. It excludes the `weather_code` and `season` since they are categorical variables for which these summary statistics cannot easily be calculated. Next, we explore each of the columns in greater detail.

```{r echo = FALSE}
summary_table_train %>%
  mutate(cnt = format(round(cnt, 0), big.mark = ","),
         t1 = format(round(t1, 2)),
         t2 = format(round(t2, 2)),
         hum = format(round(hum, 2)),
         wind_speed = format(round(wind_speed, 1)),
         is_holiday = format(round(is_holiday, 2)),
         is_weekend = format(round(is_weekend, 2))) %>%
  as_tibble() %>%
  knitr::kable( caption = "Summary Statistics")
```



### Journey Count

The `cnt` column reports the number of new bike journeys for each hourly observation. As the table of summary statistics illustrates, it cannot take on negative values since the minimum number of new bike journeys in an hour is zero.

The figure below illustrates the distribution of the values of the `cnt` variable. The left panel shows the `cnt` variable as in the dataset. The distribution is positively skewed, with many values slightly above zero and a long tail trailing to the right. As the right panel shows, after `cnt` is log-transformed, the new distribution appears to be bimodal, with a smaller peak and then a larger peak.

As discussed in the subsections which follow, the peaks in the distribution can be explained by splitting the data according to day of the week and time of day.

```{r echo = FALSE, fig.height = 3.8, fig.width = 7.333333, fig.cap = "Distribution of Observations by Number of Journey (`cnt`)"}
journey_dist
```


### Day of the Week

Demand for transport in London, as in other large cities, is heavily dependent on the day of the week. Many people live in the wide commuter belt around the city, meaning demand for commuter transport to get to the city centre peaks during the working week. We would expect to observe a similar pattern for shared bikes in the city as commuters cycle to or from work.

```{r}

```

The figure below illustrates exactly this. The left panel shows the distribution of the journey count variable (`cnt`) after log-transformation, separately for each day. The bimodal shape seen in the figure above only appears during the days of the working week. Interestingly, the bimodal distribution appears stable across the working days. The distributions for Saturdays, Sundays, and Holidays appears to have a single peak and similar shapes (once accounting for the relative prevalence of Saturdays and Sundays compared to Holidays).

```{r}
dist_byday
```

On the right, two further charts show the proportion of journey on each day and the average number of journeys per day. Bikes are mostly used in the middle of the working week (Tuesday to Thursday) and least often on holidays.


### Time of Day

The previous figure shows the number of new bike journeys follows a bimodal distribution during the working week. As we see from the figure below, this distribution is explained by the time of day. Between Monday and Friday, splitting the data into 'day', defined as between 6am and 11pm, and 'night', defined as the remaining hours, separates the observations into two distinct distributions. At weekends and on holidays, however, the two distributions overlap leading to the single, wide-peaked distributions seen above.

```{r}
dist_byday_bytime
```

The next figure breaks the distributions down further by hour of the day. We see a scatterplot of the number of new trips started in each hour of the day for every observation in the period. The points are coloured according to whether the hour is during a working or non-working day.

```{r , fig.height = 4, fig.width = 7.333333, echo = FALSE}
avgplot
```

The solid lines illustrate the loess best-fit lines. As anticipated by the analysis above, there are distinct patterns: one for working days and one for weekends and holidays. During working days, there are two peaks in bike demand, one between roughly 7am and 9am and the other between 5pm and 8pm. In contrast, at weekends and on holidays bike use follows a smooth curve, reaching its greatest demand in the middle parts of the day.

Clearly, the time of day is crucial for predicting the demand for shared bikes. The distribution of the `cnt` variable splits into two according to whether it is daytime or nighttime and this again splits according to whether or not it is the working week. In other words, there is a tangible relationship between the impact of the day of the week and the impact of the time of day on the demand for bikes. As I describe in the [Methodology] section below, the modelling approach in this paper takes this interdependence into account.  


### Month and Season

The month and season will also affect demand for shared bikes. The impact of the month or season on demand is likely related to that of the weather. However, while the weather can be changeable from day to day or even hour to hour, the calendar month or season is highly predictable. For this reason, we would expect both the calendar month / season and the weather data to aid prediction, since people's behaviour will be generally influenced by the season and then additionally so by changes in the weather.

The figure below plots the average number of bike journeys per hour for the three winter months (December to February) and the three summer months (June to August), separately for working and non-working days. During rest days, the number of bike journeys is very sensitive to the season: there are many more bike journeys in the summer than the winter. During working days, we see a similar effect, but interestingly less variability in the number of journeys during morning rush hour than in the evening.

```{r , fig.height = 4, fig.width = 7.333333, echo = FALSE}
monthplot
```

Additionally, we might expect to see longer-run trends in shared bike demand, as people's taste for cycling changes. For instance, perhaps due to a gradual improvement in road safety infrastructure for cyclists more people decide to cycle to and from work. For the sake of brevity, and since we have less than 2 years of training data, I will not explore that further here. As the [Methodology] section describes, I nevertheless include a year dummy variable as a model feature.


### Weather Effects

The dataset includes five features relating to the weather. Weather varies with the season of the year; however, all else equal, we would still expect bike demand to rise with good weather and fall with bad weather. This is what we see in the figure below. Particularly in spring and summer, regardless of whether or not it is a working day, a higher than average temperature for that time of year tends to lead to greater bike demand.

```{r}
temphourplot
```


The figure above relies on 'feels like' temperature, though the same pattern is witnessed for the real temperature. As the plot below illustrates, there is a very high degree of correlation between the two temperature metrics: usually the 'feels like' temperature *reduces* the reported real temperature somewhat in winter / early spring months. Despite the high correlation between the two measures, there remains some difference between the two temperature measures. The models described in the [Methodology] therefore include both as features.

```{r}
tempplot
```

The grid of scatterplots below show how bike demand varies with the remaining three weather features in the dataset: weather code, humidity, and wind speed. We see limited evidence of strong correlation between any of the weather features and the number of bike journeys, at least without conditioning on other variables. The humidity seems to have some predictive power: at low levels of humidity, the number of bike journeys is consistently high. The wind speed by contrast shows almost no predictive power. In fact, counterintuitivity, at higher wind speeds there tend to be more more bike journeys, suggesting that there is a confounding factor.

```{r fig.height = 3.2, fig.width = 7.333333, echo = FALSE}
weatherplots
```

### Summary

The dataset contains a number of features which appear to be useful for predicting the number of new bike journeys each hour. Time-related features appear particularly important: the day of the week (specifically, whether or not it is a working day) and the hour of the day combine to form a powerful predictor. Also important is the effect of the seasons, leading in particular to lower bike demand on evening commutes and at weekends.

Weather-related features individually show little predictive power. However, when used together and with the time features, they will add predictive power beyond usual seasonal patterns. Building on these observations, the next section describes the modelling approach used for prediction.


## Methodology


### Objective

The overall objective of this paper is to construct an algorithm to predict the demand for shared bikes in London. I construct various models and compare their predictions using a variation of the standard RMSE, the **Root Mean Squared Logarithmic Error** or **RMSLE**.

The RMSE is by far the most common loss function used for regression-type prediction problems. It is defined as follows:

\[RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{i} (c_{i} - \hat{c}_{i})^2},\]

where \(c_{i}\) and \(\hat{c}_{i}\) represent the actual and predicted `cnt` for each observation date-time \(i\). 
In this paper, I use its variant, the RMSLE, defined as:

\[RMSLE = \sqrt{\frac{1}{N}\displaystyle\sum_{i} (\log(c_{i} + 1) – \log(\hat{c}_{i} + 1))^2}.\]

This function differs from the RMSE in that, before calculating the difference, or error, it adds 1 to the actual and predicted outcomes and then calculates their natural logarithms. Substituting in \(y_{i} = \log(c_{i} + 1)\) and \(\hat{y}_{i} = \log(\hat{c}_{i} + 1)\), we see that if our outcome variable is not `cnt` but its log-transformed value \(y\), then the formula reduces to the usual RMSE.

When using a count variable, it is usual to perform a log-transformation since it treats the original count variable as coming from the Poisson family of distributions, which are distributions commonly used to represent count or frequency data.

It is common also to add a number to the outcome variable before converting it to logarithms, since this prevents the transformation from being undefined for zero-valued outcomes (since function \(\log(x)\) function is undefined at \(x=0\)). I follow the Kaggle shared bike competition in adding 1 as opposed to some other value. Since adding one is so common, R includes a built in function `log1p()` for calculating this logarithm.

Using the RMSLE for evaluating model performance is a natural choice if the model is trained using the log-values of the dependent variable. From the equations above, we see that calculating the RMSLE amounts to skipping the step of transforming the predicted outcomes \(\hat{y}_{i}\) back to the same units as the original actual outcomes before calculating the RMSE. This makes sense since the model is trained to best-fit the outcomes \(y_{i}\) and not `cnt`.


### Feature Preparation

Before running any models, I make some changes to the features provided in the original dataset:

1. An important variable in the original dataset is the `timestamp`. From it, I extract the year, the month, the day of the week, and the hour of the day. Since I have created these new features, I do not use the `timestamp` variable in my analysis. I treat all time-date related features as *categorical factors* in my analysis. As a result, each level of a factor is treated as a separate 0/1 dummy variable. Since `season` is a linear combination of three months, for simplicity I do not use `season` in my modelling. I also do not use the day of the month in my modelling.

2. Rather than treating `is_weekend`, `is_holiday` and the day of the week as separate features, I create a new 8-levelled factor variable from these three, which follows the 8 levels I use in the exploratory analysis above. If a given day of the week falls on a holiday, I recode the day of the week as "Holiday"; otherwise, I keep the day of the week (e.g. "Monday") as given. I call this new factor variable `wday_holiday` in my code, since it extends my `wday` feature for the day of the week.

3. The `weather_code` in the data takes on 7 different values, as the chart in the [Weather Effects] shows. However, since some levels are more prevalent than others, to ease combining weather with other features to form new cross-effect terms, I create a new `weather` factor taking on three levels: "Clear", "Cloudy", and "Rain".

4. Finally, I create a new `is_high_temp` feature which reports whether the "feels-like" temperature of a given observation is "High" or "Low" compared to the average for that hour-month pair. This simple dummy variable is used to incorporate the findings relating to higher or lower than average temperature seen in the figure in the [Weather Effects] section. 

Next, I describe the preprocessing steps I take before running machine learning models.


### Preprocessing of Features

#### Pre-Calculating Group Averages

Ideally the machine learning models I train would include features composed by combining different features, such as combinations of day of the week with the week and the month of the year. Doing this a lot, however, dramatically slows down the processing time required to train the algorithms.

For this reason, I create new features which include the averages for the number of new bike journeys for different groups of features. More specifically, since I treat the `cnt` variable as a Poisson variable, I take the mean of \(\log(1 + count)\) for each group according to certain groupings. This approach is similar to the averageing approach adopted in the Movielens project, which also involves calculating group means to avoid having to estimate a regression directly with many thousands of different features. See the HarvardX *Data Science Professional Certificate* [Course Text](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems) for more details on this.

After some exploration of the feasibility of different group combinations, I pre-calculate average log-counts for groups grouped by:

1. Hour (`hour`) and day of the week (`wday_holiday`);
2. Month (`month`) and day of the week (`wday_holiday`);
3. Hour (`hour`), month (`month`), and day of the week (`wday_holiday`);
4. Hour (`hour`), day of the week (`wday_holiday`), and relative temperature (`is_high_temp`);
5. Hour (`hour`), month (`month`), and relative temperature (`is_high_temp`); and
6. Hour (`hour`), day of the week (`wday_holiday`), and weather type (`weather`).

All these groupings other than the second acknowledge the likely importance of hour as a predictor. They also focus on those other predictors which, based on the exploratory analysis above, I would expect to have more predictive power. When training the model, I calculate these group averages on a subset of the `training` dataset, and then test different algorithms on the remaining portion of the `training` dataset. This permits these group averages to mimic coefficients estimated on the subset of the `training` set.

Much more work could be done to refine these groupings, and to test out which combinations are worth including instead of or in addition to those listed here. This paper does not attempt to trial different combinations, and I leave this for future work. Instead, it simply includes all these features in all modelling approaches described next.


#### Centering and Scaling Features

The penalized regression models described in the [Modelling Approach] section below require that the features matrix be fully centred and scaled before the model fitting algorithm will work. A final step I take before training any algorithms is therefore to centre and scale all features, including the group averages described above, by subtracting the mean from each observation for each feature and by dividing by the standard deviation. In order to efficiently do this, I use the `preProcess()` function from the caret package described in the [course text](https://rafalab.github.io/dsbook/caret.html).


### Modelling Approach

#### Algorithm Testing and Validation

To evaluate the performance of my final algorithm, I first split the original dataset into two parts: one subset for training and testing algorithms (`training`) and the other for validating my final algorithm (`validation`). I only use the `validation` set at the very end of my analysis, after having tested and selected among models on subsets of the `training` dataset.

For simplicity, the `training` dataset represents the first 90% of observations, ordered by `timestamp`, and the `validation` dataset represents the remaining 10%. The choice of 10% is fairly arbitrary, but the decision to set aside the final time-ordered observations is designed to simplify the computation.

For comparing models, one approach would be to use k-fold cross-validation, splitting the `training` set into k non-overlapping subsets for training, with each subset's complement being used for testing. An average of the RMSLEs calculated on each test subset would then constitute a better measure of the *true* RMSLE for a particular model. These true RMSLEs could then be compared to select the best performing model.

Despite the benefits of the k-cross-validation approach, for simplicity I slice the `training` set into two parts: an `internal_test` subset and an `internal_train` subset. The `internal_test` subset arbitrarily represents the final 10% of the `training` set and the `internal_train` set represents the remaining earlier `training` observations. I choose the final part of the `training` set for testing due to concerns over potential over-fitting when using future observations to fit past observations. However, I note that I am not entirely consistent in this approach, since as I explain below I use regular cross-validation on the `internal_train` set to select a penalty term.


#### Baseline: Ordinary Linear Regression

As a baseline, I perform ordinary least squared regression (OLS). The fitted model I obtain may be described as follows:

\[\hat{y}_{i} = \hat{b}_{0} + \hat{b}_{g} + \hat{b}_{t} + \hat{b}_{w},\]

In this equation, \(\hat{b}_{0}\) is the estimated intercept term which will equal the mean value of \(\hat{y}_{i}\) in the dataset, since the other features are all centered. \(\hat{b}_{g}\) represents the set of 6 group average features described in the [Pre-Calculating Group Averages] section. The next term \(\hat{b}_{t}\) represents a series of time-date related fixed effects for each hour, each calendar month, each day of the week (`wkday_holiday`), and each year. The final term \(b_{w}\) represents weather-related variables, where those which are categorical variables such as `weather_code` and `is_high_temp` take on a series of fixed effects. In total, the model includes 60 features, excluding the implicit fixed-effect levels.

Note that the outcome variable is \(\hat{y}_{i} = \log(\hat{c}_{i} + 1)\), where \(\hat{c}_{i}\) is the new bike journey `cnt` variable. This setup, transforming the count variable via what is known as a *link* function, means that the model falls into the category of generalized linear models (GLM), where a function of the original dependent variable is a linear function of the features. Although I model using OLS, an alternative method would use the Poisson version of GLM directly.

I estimate the OLS regression coefficients using the `internal_train` dataset and calculate an out-of-sample RMSE on the `internal_test` dataset. I use this RMSE to compare the predictive ability of OLS against other models described model.


#### Regularization: Ridge Regression

The next model trained is ridge regression. Ridge regression is another name for the type of regularization followed in the movie recommendation system in the [course text](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems).

In ridge regression, we seek the set of coefficients \(\hat{b}_{p}\) for parameters \(p\) which minimizes the following cost function:

\[\sum_{i}(y_{i} - b_{0} - b_{g} - b_{t} - b_{w})^2 + \lambda(\sum_{g}b_{g}^2 + \sum_{t}b_{t}^2 + \sum_{w}b_{w}^2),\]

where the terms have the same meaning as in the previous subsection. This expression represents the usual mean squared error loss function, but with a second term of \(\lambda\) multiplied by the square of the coefficient on each feature. This second term *increases* the overall loss: the larger the \(\lambda\), the greater the penalization and more coefficients are shrunk towards zero. Note that the intercept is not penalized, in order to ensure we have a unique solution.^[See Ryan Tibshirani's Data Mining Lecture Slides, Lecture 16, p. 7 (https://www.stat.cmu.edu/~ryantibs/datamining/lectures/16-modr1.pdf), last accessed 16 April 2020.] As a result, as in the case of OLS regression, the intercept will equal the mean of \(\hat{y}\) in the dataset used for estimation.

A trained model's prediction error can be decomposed into the variance of the true model and mean squared error (MSE). The MSE itself can be decomposed into a product of the model's bias (squared) and its variance. This is known as the **bias-variance trade-off**.^[See Ryan Tibshirani's Data Mining Lecture Slides, Lecture 14 (https://www.stat.cmu.edu/~ryantibs/datamining/lectures/14-reg2.pdf), last accessed 16 April 2020. See also Datacamp, Tutorial *Regularization: Ridge, Lasso and Elastic Net*, (https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net), last accessed 16 April 2020.]

The advantage of OLS is that it results in unbiased coefficient estimates - coefficient estimates which, in expectation, equal the true model parameters. This reduces the MSE. However, OLS also leads to larger variance  in the fitted model, meaning that the estimated coefficients are likely to vary a lot from dataset to dataset, and increasing the MSE. This can occur if features are highly correlated with one another or if the model contains a large number of features.

The idea of ridge regression is to reduce the model variance at the expense of introducing some bias, and in doing so reduce the MSE and therefore the prediction error. For the right values of \(\lambda\), it should improve the ability of a model trained on one dataset to predict outcomes on another dataset.

To perform ridge regression, I use the glmnet R package. Throughout, I set the `alpha` parameter to zero to select ridge regression as opposed to lasso regression which I describe below. To select the \(\lambda\) penalty term, I try 100 different \(\lambda\) values in the range between \(10^{-10}\) and \(10^{10}\). For each value of \(\lambda\), I use 10-fold cross-validation on the `internal_train` dataset to obtain an estimated MSE averaged across the 10 test folds. I then choose the \(\lambda\) one standard error away from the \(\lambda\) which minimises the MSE. This \(\lambda\) is larger than the \(\lambda\) that minimises the MSE, and therefore results in more penalization, reducing the variance of the model.

In order to compare the performance of ridge regression against OLS regression and other types of model, I fit the regularized model with the optimal choice of \(lambda\) to the full `internal_train` dataset. I then compute an RMSE (in terms of the outcome variable \(y\) or RMSLE in terms of the `cnt` variable) by applying the trained model to the `internal_test` dataset.


#### Regularization: Lasso Regression

The final model I train is lasso regression. Lasso regression is similar to ridge regression, in that it is also a way of penalizing coefficients towards zero. Its difference from ridge regression is that lasso also leads to some features' being excluded entirely from the regression. In other words, lasso's solution to the bias-variance trade-off is to reduce the coefficients on some features to zero, reducing the number of features. This reduces the complexity of the model, aiding model interpretation, and also lowers the variance.

In lasso regression, we seek the set of coefficients \(\hat{b}_{p}\) for parameters \(p\) which minimizes the following cost function:

\[\sum_{i}(y_{i} - b_{0} - b_{g} - b_{t} - b_{w})^2 + \lambda(\sum_{g}\mid b_{g}\mid + \sum_{t}\mid b_{t}\mid + \sum_{w}\mid b_{w}\mid),\]

The only difference between the lasso and ridge regression cost functions is that instead of penalizing using *squared* coefficient terms - as in ridge - the lasso approach instead uses the *absolute* values of coefficients in the penalty term.

As for ridge, I use the glmnet package to estimate lasso. I select lasso by setting the `alpha` parameter to 1. I also use the same 100 \(\lambda\) values in the range between \(10^{-10}\) and \(10^{10}\). As before, for each \(\lambda\), I use 10-fold cross-validation on the `internal_train` dataset to obtain an MSE, and select the \(lambda\) one standard error away from the \(lambda\) that minimises the MSE. This \(\lambda\) results in more penalization, and therefore more features will be removed than at the \(\lambda\) that minimises the MSE.

To compare the models, again I fit the model again using the final choice of \(\lambda\) to the full `internal_train` dataset and calculate the RMSE on the `internal_test` set.



## Results

### Baseline Results

The following plot shows the size of the coefficients on each feature in the OLS regression, excluding the intercept. From this, we see that the features with the largest intercepts are mostly those we might expect: the pre-calculated group averages. The `cnt_month_wkday_holiday` has very little predictive power, as might be expected because it excludes the hour variable. The feature with the largest coefficient is the group averages for hour, month, and day of the week. This was anticipated by the figure in the [Month and Season] subsection. 

Other than the group features, the hours remain those features with the most predictive power. The days of the week have very little to add, once the group averages with the hour and month are already included.

The estimated intercept is `r format(as.matrix(results_lm$fit$coefficients)[1,], digits = 3)`, which equals the mean of \(\hat{y}_{i}\) and is substantially larger than the nearest coefficient.

Using the fitted model to predict the outcome in the `internal_test` dataset, I obtain an RMSE of `r format(results_lm$rmsle_out, digit = 4)`.


```{r echo = FALSE}
tibble(Feature = rownames(as.matrix(results_lm$fit$coefficients)),
       Estimate = results_lm$fit$coefficients) %>%
  slice(2:length(Feature)) %>%
  mutate(Feature = reorder(Feature, Estimate, FUN = "identity")) %>%
  ggplot(aes(Feature, Estimate)) + geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(-0.75, 1)) +
  theme_parts + theme(axis.text.x = element_text(angle = 90, hjust = TRUE, vjust = FALSE),
                      aspect.ratio = 2/7)
```

#### Ridge Regression

The figure below illustrates how the average MSE from 10-fold cross-validation varies for different value of \(\lambda\) for ridge regression. We see that the MSE is minimised when \(\log(\lambda)\) is less than zero, or in other words \(\lambda\) is less than one. The \(\lambda\) I choose to use going forward is \(\lambda\) = `r results_L2$cv$lambda.1se`, close to zero. Re-estimating the model on the full `internal_train` dataset, using my choice of \(\lambda\), I obtain an RMSE of `r format(results_L2$rmsle_out, digit = 4)`.

```{r echo = FALSE}
plot(results_L2$cv)
results_L2$rmsle_out
```


#### Lasso Regression

Likewise, the figure below illustrates the average MSE for lasso from 10-fold cross-validation for different value of \(\lambda\). We see that the MSE is minimised when \(\log(\lambda)\) is less than zero, or in other words \(\lambda\) is less than one. Across the top of the figure we can also see the number of features the model includes, decreasing rightwards across the figure. The \(\lambda\) I choose to use is \(\lambda\) = `r results_L1$cv$lambda.1se`, again close to zero.

```{r echo = FALSE}
plot(results_L1$cv)
```

Re-estimating the model on the full `internal_train` dataset, using my choice of \(\lambda\), I obtain an RMSE of `r format(results_L1$rmsle_out, digit = 4)`. The chart below illustrates the coefficient estimates in the final lasso model. We see that lasso leaves `r tibble(Estimate = as.matrix(results_L1$fit$beta)) %>% filter(Estimate != 0) %>% count() %>% pull()` of the original 60 features in the model.


```{r echo = FALSE}
tibble(Feature = rownames(results_L1$fit$beta),
       Estimate = as.matrix(results_L1$fit$beta)) %>%
  filter(Estimate != 0) %>%
  mutate(Feature = reorder(Feature, Estimate, FUN = "identity")) %>%
  ggplot(aes(Feature, Estimate)) + geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(-0.75, 1)) +
  theme_parts + theme(axis.text.x = element_text(angle = 90, hjust = TRUE, vjust = FALSE),
                      aspect.ratio = 2/7)
```


## Concluding Remarks


One idea would be to use different \(\lambda\) for different parameters. For instance, we might not want to regularize the coefficients on the group average features at all, but only those on the other features. This is something that the glmnet package permits.^[See the Glmnet Vignette, p. 14 for further details. (https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf), last accessed 16 April 2020.]

## References

https://towardsdatascience.com/predicting-no-of-bike-share-users-machine-learning-data-visualization-project-using-r-71bc1b9a7495

https://www.imachordata.com/do-not-log-transform-count-data-bitches/

https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/07-0043.1
http://byrneslab.net/classes/biol607/readings/o'hara_and_kotze_do_not_log_transform.pdf


https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23

https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/
https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning
https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/ch04.html#table_encoding_workclass_feature
https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23
