---
title: "Predicting Shared Bike Usage in London"
author: "Tom Dorrington Ward"
date: "23 April 2020"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
  html_notebook:
    theme: readable
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
# Import required libraries
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!exists("rmse_validation")) {source("bike_analysis_report.R")}
```


# Introduction

In this project, I use data science and machine learning techniques learned on the HarvardX [*Data Science Professional Certificate*](https://www.edx.org/professional-certificate/harvardx-data-science) programme on [edX](https://www.edx.org/) to predict the demand for shared bikes in London. This paper forms my final project for the capstone course.

Kaggle, the online data science community, offers a competition to [forecast shared bike demand](https://www.kaggle.com/c/bike-sharing-demand) on a scheme in Washington DC, the [Capital Bikeshare]( https://www.capitalbikeshare.com/about).^[See the competition's page for further details (https://www.kaggle.com/c/bike-sharing-demand), last accessed 23 April 2020.] Rather than taking that widely used dataset, I use data on a similar bike scheme in London, also available from the Kaggle platform. On the one hand, I examine shared bike demand because there are already a large number of analyses out there to learn from.^[See for example the large number of notebooks attached to Kaggle's competition.] On the other hand, I choose to examine London as opposed to Washington DC, since there are fewer analyses of London’s data already out there.

The demand for shared bikes is measured as a frequency or count variable, which tallies the number of new trips started each hour. As a result, unlike a Normal random variable, it cannot take on negative values and its distribution is skewed with a long tail towards higher count values. Kaggle implicitly acknowledges the skewed shape of the distribution, since for any competition entry, before calculating the standard Root Mean Squared Error or RMSE measure of prediction performance, it first transforms the count outcome variable. Kaggle transforms both the actual count and its predicted counterpart by taking their logarithms, leading to what is known as the Root Mean Squared *Logarithmic* Error or RMSLE. Log-transforming the outcome variable is a common practice in data science, especially with count variables, which may be assumed to be Poisson (or similar) distributed. I define the RMSLE in the [Methodology] section of this report.

In this paper, I compare a number of machine learning models for predicting bike demand. First, I train a standard linear model to predict the log-transformed count outcome. Second, I train two penalized linear models - Ridge and Lasso - to see whether I can improve the prediction. As I explain in the [Methodology], the intuition behind these two models takes into account the bias-variance tradeoff: they reduce model variance at the expense of greater bias, with the overall aim of improving prediction.

I compare the results of the various models using the RMSLE as the measure of goodness-of-fit, where the error, or residual, is calculated by taking the difference between the log-transformed actual count variable and its predicted counterpart.

The remainder of this paper proceeds as follows:

1.	First, the [Exploring the Dataset] section describes and explores the London bike sharing dataset in detail. The results of this exploration feed directly into the choice of predictors in the modelling.

2.	Next, the [Methodology] section builds on the previous data exploration step by describing the modelling approach. It begins by more formally describing the objective of this paper and defines the RMSLE loss function. It then describes the cleaning and feature preparation steps taken, before describing the particular machine learning models I apply.

3.	Next, the [Results] section describes the results of the various modelling approaches. It also presents a final RMSLE for prediction on a previously *untouched* dataset.

4.	Finally, the [Concluding Remarks] section offers an overall assessment of the work in this paper and areas for future work.


# Exploring the Dataset

The analysis in this paper uses a time series dataset downloaded from [Kaggle](https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset), which counts the number of new trips on London’s cycle hire scheme each hour for the period from `r format(summary_stats_full$start_date, "%d %B %Y")` to `r format(summary_stats_full$last_date, "%d %B %Y")`.

The bike scheme in London is officially called [Santander Cycles]( https://tfl.gov.uk/modes/cycling/santander-cycles) (previously Barclays Cycle Hire), though the bikes are colloquially known as “Boris bikes”, after Boris Johnson, London’s mayor when the scheme was implemented. The scheme allows any member of the public to easily hire and ride a bike for a one-direction trip by collecting a bike from a [docking station](https://tfl.gov.uk/modes/cycling/santander-cycles/find-a-docking-station?intcmp=2321) and returning it to another station located around central London. As of today, there is a network of 750 docking stations around the UK’s capital with over 11,500 bikes.

Besides counting bike trips, the dataset also includes a number of other variables which are likely useful for predicting the number of trips, including the dates of bank holiday and recorded weather data.

Although I download the full dataset from Kaggle, the dataset’s [webpage]( https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset) makes clear the original sources of the data. The bike data is originally gathered from [Transport for London (TfL)](https://cycling.data.tfl.gov.uk/), London’s transport authority, the dates of bank holidays from the [UK Government website]( https://www.gov.uk/bank-holidays), and the weather data from www.freemeteo.com.


## Descriptive Statistics

Overall, the dataset contains \(n=\) `r format(summary_stats_full$total_obs, big.mark = ",")` observations, each representing a one-hour period between `r format(summary_stats_full$start_date, "%d %B %Y")` and `r format(summary_stats_full$last_date, "%d %B %Y")`.

For the benefit of assessing the performance of the final algorithm, however, I partition the original dataset into two parts: a `training` dataset representing the first 90% of observations, ordered by `timestamp`, and a `validation` dataset representing the remaining 10%. The choice of 10% is fairly arbitrary, but the decision to set aside the final time-ordered observations is designed to simplify the calculations.

The exploratory analysis in this paper exclusively examines the `training` dataset, and the paper only turns to the `validation` set for calculating an out-of-sample measure of predictive performance using the final trained algorithm. The `training` set contains \(n=\) `r format(summary_stats_train$total_obs, big.mark = ",")` observations, each representing a one-hour period between `r format(summary_stats_train$start_date, "%d %B %Y")` and `r format(summary_stats_train$last_date, "%d %B %Y")`. It spans `r format(summary_stats_train$total_days, big.mark = ",")` different days, counting `r format(summary_stats_train$total_trips, big.mark = ",")` bike journeys.

Table \ref{tab:tab1} describes the ten different columns included in the dataset. Besides time, date, season and journey data, the dataset includes a number of columns containing meteorological data.

```{r tab1, echo = FALSE, message = FALSE, warning = FALSE}
data.frame(
  Name = c("Timestamp", "Journey Count", "T1", "T2", "Humidity", "Wind Speed", "Weather Code", "Is Holiday", "Is Weekend", "Season"),
  Variable = c("`timestamp`", "`cnt`", "`t1`", "`t2`", "`hum`", "`wind_speed`", "`weather_code`", "`is_holiday`", "`is_weekend`", "`season`"),
  Description = c("Date-time of observation, hourly",
                  "Count of new bike shares started in an hour",
                  "Real temperature in ̊C",
                  "'Feels like' temperature in ̊C",
                  "Humidity as a percentage",
                  "Wind speed in km per hour",
                  "1: Clear; 2: Few clouds; 3: Broken clouds; 4: Cloudy; 7: Rain; 10: Thunderstorm; 26: Snowfall",
                  "Boolean field - 1 holiday, 0 non-holiday",
                  "Boolean field - 1 weekend, 0 non-weekend",
                  "Category field, meteorological season - 0 spring, 1 summer, 2 autumn, 3 winter")) %>%
  knitr::kable("latex", booktabs = T, caption = "Variable Descriptions") %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

Table \ref{tab:tab2} reports standard summary statistics for each quantitative variable in the `training` dataset, besides the `timestamp`. It excludes the `weather_code` and `season` since they are categorical variables for which these summary statistics cannot easily be calculated. Next, we explore each of the columns in greater detail.

```{r tab2, echo = FALSE, message = FALSE, warning = FALSE}
summary_table_train %>%
  mutate(cnt = format(round(cnt, 0), big.mark = ","),
         t1 = format(round(t1, 2)),
         t2 = format(round(t2, 2)),
         hum = format(round(hum, 2)),
         wind_speed = format(round(wind_speed, 1)),
         is_holiday = format(round(is_holiday, 2)),
         is_weekend = format(round(is_weekend, 2))) %>%
  as_tibble() %>%
  knitr::kable("latex", booktabs = T, caption = "Summary Statistics") %>%
  kable_styling(position = "center", latex_options = c("striped"))

```



## Journey Count

The `cnt` column reports the number of new bike journeys for each hourly observation. As the table of summary statistics illustrates, it cannot take on negative values since the minimum number of new bike journeys in an hour is zero.

Figure 1 illustrates the distribution of the values of the `cnt` variable. The left panel shows the `cnt` variable as in the dataset. The distribution is positively skewed, with many values slightly above zero and a long tail trailing to the right. As the right panel shows, after `cnt` is log-transformed, the new distribution appears to be bimodal, with a smaller peak and then a larger peak.

As discussed in the subsections which follow, the peaks in the distribution can be explained by splitting the data according to day of the week and time of day.

```{r fig1, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3.8, fig.width = 7.333333, fig.cap = "Distribution of Number of New Bike Journeys (`cnt`)"}
journey_dist
```


## Day of the Week

Demand for transport in London, as in other large cities, is heavily dependent on the day of the week. Many people live in the wide commuter belt around the city, meaning demand for commuter transport, such as trains and buses, peaks during the working week. We would expect to observe a similar pattern for shared bikes in the city as commuters cycle to or from work.

Figure 2 illustrates exactly this. The left panel shows the distribution of the journey count variable (`cnt`) after log-transformation, separately for each day. The bimodal shape seen in Figure 1 only appears during the days of the working week. Interestingly, the bimodal distribution appears stable across the working days. The distributions for Saturdays, Sundays, and Holidays appears to have a single peak and similar shapes (once accounting for the relative prevalence of Saturdays and Sundays compared to Holidays).

On the right, two further charts show the proportion of journey on each day and the average number of journeys per day. Bikes are mostly used in the middle of the working week (Tuesday to Thursday) and least often on holidays.

```{r fig2, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Distribution of Number of New Bike Journeys (`cnt`) by Day of the Week"}
dist_byday
```


## Time of Day

Figure 2 shows the number of new bike journeys follows a bimodal distribution during the working week. As we see from the next figure, Figure 3, this distribution is explained by the time of day. Between Monday and Friday, splitting the data into 'day', defined as between 6am and 11pm, and 'night', defined as the remaining hours, separates the observations into two distinct distributions. At weekends and on holidays, however, the two distributions overlap leading to the single, wide-peaked distributions we see in Figure 2.

```{r fig3, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Distribution of Number of New Bike Journeys (`cnt`) by Day of the Week and Time of Day"}
dist_byday_bytime
```

Figure 4 breaks the distributions down further by hour of the day. We see a scatterplot of the number of new trips started each hour, covering every observation in the period. The points are coloured according to whether the hour is during a working day (blue) or non-working day (red).

```{r fig4, fig.height = 4, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Scatterplot of Number of New Bike Journeys (`cnt`) by Hour of Day and Type of Day"}
avgplot
```

The solid lines illustrate the loess best-fit lines. As anticipated by the analysis above, there are distinct patterns: one for working days and one for weekends and holidays. During working days, there are two peaks in bike demand, one between roughly 7am and 9am and the other between 5pm and 8pm. In contrast, at weekends and on holidays bike use follows a smooth curve, reaching its greatest demand in the middle part of the day.

Clearly, the time of day is crucial for predicting the demand for shared bikes. The distribution of the `cnt` variable splits into two according to whether it is daytime or nighttime and this again splits according to whether or not it is the working week. In other words, there is a tangible interaction between the impact of the day of the week and the impact of the time of day on the demand for bikes. As I describe in the [Methodology] section below, the modelling approach in this paper takes this interdependence into account.  


## Month and Season

The month and season will also affect demand for shared bikes. The impact of the month or season on demand is likely related to that of the weather. However, while the weather can be changeable from day to day or even hour to hour, the calendar month or season is highly predictable. For this reason, we would expect both the calendar month / season and the weather data to aid prediction, since people's behaviour will be generally influenced by the season and then additionally so by changes in the weather.

Figure 5 plots the average number of bike journeys per hour for the three winter months (December to February) and the three summer months (June to August), separately for working and non-working days. During rest days, the number of bike journeys is very sensitive to the season: there are many more bike journeys in the summer than the winter. During working days, we see a similar effect, but interestingly less variability in the number of journeys during morning rush hour than in the evening.

```{r fig5, fig.height = 4, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Average Number of New Bike Journeys (`cnt`) by Hour, Month, and Type of Day"}
monthplot
```

Additionally, we might expect to see longer-run trends in shared bike demand, as people's taste for cycling evolves. For instance, perhaps due to a gradual improvement in road safety infrastructure for cyclists more people decide to cycle to and from work. For the sake of brevity, and since we have less than 2 years of training data, I will not explore that further here. As the [Methodology] section describes, I nevertheless include a year dummy variable as a model feature to capture this potential effect.


## Weather Effects

The dataset includes five features related to the weather. Weather varies with the season of the year; however, all else equal, we would still expect bike demand to rise with good weather and fall with bad weather. This is what we see in Figure 5. Particularly in spring and summer, regardless of whether or not it is a working day, a higher than average temperature for that time of year tends to lead to greater bike demand.

```{r fig6, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Scatterplot of Number of New Bike Journeys (`cnt`) by Season and 'Feels-Like' Temperature"}
temphourplot
```


Figure 6 relies on 'feels like' temperature, though the same pattern is witnessed for the real temperature. As Figure 7 illustrates, there is a very high degree of correlation between the two temperature metrics: usually the 'feels like' temperature *reduces* the reported real temperature somewhat in winter / early spring months. Despite the high correlation between the two measures, there remains some difference between the two temperature measures. The models described in the [Methodology] therefore include both as features.

```{r fig7, fig.height = 3.2, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Comparison of Temperature Features"}
tempplot
```

The grid of scatterplots in Figure 8 shows how bike demand varies with the remaining three weather features in the dataset: weather code, humidity, and wind speed. We see limited evidence of strong correlation between any of the weather features and the number of bike journeys, at least without conditioning on other variables. The humidity seems to have some predictive power: at low levels of humidity, the number of bike journeys is consistently high. The wind speed by contrast shows almost no predictive power. In fact, counterintuitivity, at higher wind speeds there tend to be more more bike journeys, suggesting that there is a confounding factor.

```{r fig8, fig.height = 3.2, fig.width = 7.333333, echo = FALSE, fig.cap = "Scatterplots of Number of New Bike Journeys (`cnt`) for Weather  Features"}
weatherplots
```

## Summary

The dataset contains a number of features which appear to be useful for predicting the number of new bike journeys each hour. Time-related features appear particularly important: the day of the week (specifically, whether or not it is a working day) and the hour of the day combine to form a powerful predictor. Also important is the effect of the seasons, leading in particular to lower bike demand on evening commutes and at weekends.

Weather-related features individually show little predictive power. However, when used together and with the time features, they will add predictive power beyond usual seasonal patterns. Building on these observations, the next section describes the modelling approach used for prediction.


# Methodology


## Objective

The overall objective of this paper is to construct an algorithm to predict the demand for shared bikes in London. I construct various models and compare their predictions using a variation of the standard RMSE, the *Root Mean Squared Logarithmic Error*, or *RMSLE*.

The RMSE is by far the most common loss function used for regression-type prediction problems. It is defined as follows:

\[RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{i} (c_{i} - \hat{c}_{i})^2},\]

where \(c_{i}\) and \(\hat{c}_{i}\) represent the actual and predicted `cnt` for each observation date-time \(i\). In this paper, I use its variant, the RMSLE, defined as:

\[RMSLE = \sqrt{\frac{1}{N}\displaystyle\sum_{i} (\log(c_{i} + 1) - \log(\hat{c}_{i} + 1))^2}.\]

The RMSLE function differs from the RMSE in that, before calculating the difference, or error, it adds 1 to the actual and predicted outcomes and then calculates their natural logarithms. Substituting in \(y_{i} = \log(c_{i} + 1)\) and \(\hat{y}_{i} = \log(\hat{c}_{i} + 1)\), we see that if our outcome variable is not `cnt` but its log-transformed value \(y\), then the formula reduces to the usual RMSE.

When using a count variable, it is common to perform a log-transformation. It is common also to add a number to the outcome variable before converting it to logarithms, since this prevents the transformation from being undefined for zero-valued outcomes (since \(\log(x)\) is undefined at \(x=0\)). I follow the Kaggle shared bike competition in adding 1 as opposed to some other value.^[See the evaluation page for the Kaggle competition (https://www.kaggle.com/c/bike-sharing-demand/overview/evaluation), last accessed 23 April 2020.] Since adding one is so common, R includes a built-in function `log1p()` for calculating this logarithm.


## Feature Preparation

Before training any models, I make some changes to the features provided in the original dataset:

1. An important variable in the original dataset is the `timestamp`. From it, I extract the year, the month, the day of the week, and the hour of the day. Since I have created these new features, I do not use the `timestamp` variable in my analysis. I treat all time-date related features as *categorical factors* in my analysis.^[See Chapter 4 of  *Introduction to Machine Learning with Python*, Andreas C. Müller and Sarah Guido, O'Reilly, for a detailed discussion of feature engineering (https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/ch04.html), last accessed 23 April 2020.] As a result, each level of a factor is treated as a separate 0/1 dummy variable. Since each `season` is a linear combination of three month dummies, I do not use `season` in my modelling. I also do not use the day of the month in my modelling.

2. Rather than treating `is_weekend`, `is_holiday` and the day of the week as separate features, I create a new 8-levelled factor variable from these three, which follows the 8 levels I use in the exploratory analysis above. If a given day of the week falls on a holiday, I recode the day of the week as "Holiday"; otherwise, I keep the day of the week (e.g. "Monday") as given. I call this new factor variable `wday_holiday` in my code, since it extends my `wday` feature for the day of the week.

3. The `weather_code` in the data takes on 7 different values, as Figure 8 shows. However, since some levels are more prevalent than others, to ease combining weather with other features to form new interaction terms, I create a new `weather` factor taking on three levels: "Clear", "Cloudy", and "Rain".^["Clear" corresponds to `weather_code` 1; "Cloudy" corresponds to codes 2, 3, and 4; and "Rain" corresponds to codes 7, 10, and 26.]

4. Finally, I create a new `is_high_temp` feature which reports whether the "feels-like" temperature of a given observation is "High" or "Low" compared to the average for that hour-month pair. This dummy variable is used to incorporate the findings relating to higher or lower than average temperature seen in Figure 6.^[I have not investigated other ways of encoding temperature as a categorical variable. One way to improve the analysis might be to try alternative encodings.]

Next, I describe the preprocessing steps I take before running machine learning models.


## Preprocessing of Features

### Pre-Calculating Group Averages

Ideally the machine learning models I train would include features composed by combining different features, such as combinations of day of the week with the hour and the month of the year. Doing this a lot, however, dramatically slows down the processing time required to train the algorithms.

For this reason, I create new features which include the averages for the number of new bike journeys for different groups of features. More specifically, since I treat the `cnt` variable as a Poisson variable, I take the mean of \(\log(1 + count)\) for each group according to certain groupings. This approach is similar to the averageing approach adopted in the Movielens project, which also involves calculating group means to avoid having to estimate a regression directly with many thousands of different features. See the HarvardX *Data Science Professional Certificate* [Course Text](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems) for more details on this.^[See *Data Analysis and Prediction Algorithms with R*, Rafael A. Irizarry, (https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems), last updated 1 April 2020; last accessed 23 April 2020.]

After some exploration of the feasibility of different group combinations, I pre-calculate average log-counts for groups grouped by:

1. Hour (`hour`) and day of the week (`wday_holiday`);
2. Month (`month`) and day of the week (`wday_holiday`);
3. Hour (`hour`), month (`month`), and day of the week (`wday_holiday`);
4. Hour (`hour`), day of the week (`wday_holiday`), and relative temperature (`is_high_temp`);
5. Hour (`hour`), month (`month`), and relative temperature (`is_high_temp`); and
6. Hour (`hour`), day of the week (`wday_holiday`), and weather type (`weather`).

All these groupings other than the second acknowledge the likely importance of hour as a predictor. They also focus on those other predictors which, based on the exploratory analysis above, I would expect to have more predictive power. When training the model, I calculate these group averages on a subset of the `training` dataset, and then test different algorithms on the remaining portion of the `training` dataset. This permits these group averages to mimic coefficients estimated on the subset of the `training` set.

Much more work could be done to refine these groupings, and to test out which combinations are worth including instead of or in addition to those listed here. This paper does not attempt to trial different combinations, and I leave this for future work. Instead, it simply includes all features in all modelling approaches described next.


### Centering and Scaling Features

The penalized regression models described in the [Modelling Approach] section below require that the features matrix be standardized before the model fitting algorithm will work. A final step I take before training any algorithms is therefore to centre and scale all features, including the group averages described above, by subtracting the mean from each observation for each feature and by dividing by the standard deviation. In order to efficiently do this, I use the `preProcess()` function from the caret package described in the [course text](https://rafalab.github.io/dsbook/caret.html).


## Modelling Approach

### Algorithm Testing and Validation

To evaluate the performance of my final algorithm, I first split the original dataset into two parts: one subset for training and testing algorithms (`training`) and the other for validating my final algorithm (`validation`). I only use the `validation` set at the very end of my analysis, after having tested and selected among models on subsets of the `training` dataset.

For simplicity, the `training` dataset represents the first 90% of observations, ordered by `timestamp`, and the `validation` dataset represents the remaining 10%. The choice of 10% is fairly arbitrary, but the decision to set aside the final time-ordered observations is designed to simplify the computation.

For comparing models, one approach would be to use k-fold cross-validation, splitting the `training` set into k non-overlapping subsets for training, with each subset's complement being used for testing. An average of the RMSLEs calculated on each test subset would then constitute a better measure of the *true* RMSLE for a particular model. These true RMSLEs could then be compared to select the best performing model.

Despite the benefits of the k-cross-validation approach, for simplicity I slice the `training` set into two parts: an `internal_test` subset and an `internal_train` subset. The `internal_test` subset arbitrarily represents the final 10% of the `training` set and the `internal_train` set represents the remaining earlier `training` observations. I choose the final part of the `training` set for testing due to concerns over potential over-fitting when using future observations to fit past observations. However, I note that I am not entirely consistent in this approach, since as I explain below I use regular cross-validation on the `internal_train` set to select a penalty term.


### Baseline: Ordinary Linear Regression

As a baseline, I perform ordinary least squared regression (OLS). The fitted model I obtain may be described as follows:

\[\hat{y}_{i} = \hat{b}_{0} + \hat{b}_{g} + \hat{b}_{t} + \hat{b}_{w},\]

In this equation, \(\hat{b}_{0}\) is the estimated intercept term which will equal the mean value of \(\hat{y}_{i}\) in the dataset, since the other features are all centered. \(\hat{b}_{g}\) represents the set of 6 group average features described in the [Pre-Calculating Group Averages] section. The next term \(\hat{b}_{t}\) represents a series of time-date related fixed effects for each hour, each calendar month, each day of the week (`wkday_holiday`), and each year. The final term \(b_{w}\) represents weather-related variables, where those which are categorical variables such as `weather_code` and `is_high_temp` take on a series of fixed effects. In total, the model includes 60 features, excluding the implicit fixed-effect levels.

Note that the outcome variable is \(\hat{y}_{i} = \log(\hat{c}_{i} + 1)\), where \(\hat{c}_{i}\) is the new bike journey `cnt` variable. This setup, transforming the count variable, is similar to generalized linear models (GLM), where a function of the mean of the dependent variable's distribution may be expressed as a linear function of the features.

Although I model using OLS, a potentially better method would use the Poisson version of GLM. In this model, \(\log(\mu)\) is a linear function of the features, where \(\mu\) is the mean of the `cnt` variable's conditional distribution. Poisson regression has the benefit that it can deal with zero-valued outcomes directly, without adding a 1 to the dependent variable, thereby reducing model bias. As a result could potentially increase prediction accuracy, although in this paper I measure model accuracy after already adding 1 to the dependent variable, so it is not clear that that applies here. I save Poisson regression for future work. 

I estimate the OLS regression coefficients using the `internal_train` dataset and calculate an out-of-sample RMSE on the `internal_test` dataset. I use this RMSE to compare the predictive ability of OLS against other models described model.


### Regularization: Ridge Regression

The next model I train is ridge regression. Ridge regression is another name for the type of regularization followed in the movie recommendation system in the [course text](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems).

In ridge regression, we seek the set of coefficients \(\hat{b}_{p}\) for parameters \(p\) which minimizes the following cost function:

\[\sum_{i}(y_{i} - b_{0} - b_{g} - b_{t} - b_{w})^2 + \lambda(\sum_{g}b_{g}^2 + \sum_{t}b_{t}^2 + \sum_{w}b_{w}^2),\]

where the terms have the same meaning as in the previous subsection. This expression represents the usual mean squared error loss function, but with a second term of \(\lambda\) multiplied by the square of the coefficient on each feature. For a given set of \(\hat{b}_{p}\), the second term *increases* the overall loss compared to the least squares loss function. In order to minimize the cost function for a given \(\lambda\), the coefficients are shrunk towards zero. Note that the intercept is not penalized, in order to ensure we have a unique solution.^[See Ryan Tibshirani's Data Mining Lecture Slides, Lecture 16, p. 7 (https://www.stat.cmu.edu/~ryantibs/datamining/lectures/16-modr1.pdf), last accessed 23 April 2020.] As a result, as in the case of OLS regression, the intercept will equal the mean of \(\hat{y}\) in the dataset used for estimation.

A model's test MSE prediction error can be decomposed as the sum of the irreducible error, which is the variance of the true model's error, the model's (squared) bias, and its variance. This is known as the **bias-variance trade-off**, since fitting a model with lower variance will tend to mean higher bias and vice versa.^[See G. James et al. *An Introduction to Statistical Learning with Applications in R*, Chapters 2 and 6, Corrected 7th printing (http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf), last accessed 23 April 2020. See also Ryan Tibshirani's Data Mining Lecture Slides, Lecture 14 (https://www.stat.cmu.edu/~ryantibs/datamining/lectures/14-reg2.pdf), last accessed 23 April 2020.]

The advantage of OLS is that, assuming the true model is linear, it results in unbiased coefficient estimates, reducing the MSE. However, OLS also leads to larger variance than would a more constrained, less flexible model, meaning that the estimated coefficients are free to vary more when trained on one dataset or another.

The idea of ridge regression is to reduce the model variance at the expense of introducing some bias, and in doing so reduce the test MSE. For certain values of \(\lambda\), it can improve the ability of a model trained on one dataset to predict outcomes on another dataset.

To perform ridge regression, I use the glmnet R package.^[This is described in the Lab of Chapter 6 of G. James et al. *An Introduction to Statistical Learning with Applications in R*, Corrected 7th printing (http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf), last accessed 23 April 2020. See also the Glmnet Vignette (https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf), last accessed 23 April 2020.] Throughout, I set the `alpha` parameter to zero to select ridge regression as opposed to lasso regression which I describe below. To select the \(\lambda\) penalty term, I try 100 different \(\lambda\) values in the range between \(10^{-10}\) and \(10^{10}\). For each value of \(\lambda\), I use 10-fold cross-validation on the `internal_train` dataset to obtain an estimated MSE averaged across the 10 test folds. I then choose the \(\lambda\) one standard error away from the \(\lambda\) which minimises the MSE. This \(\lambda\) is larger than the \(\lambda\) that minimises the MSE, and therefore results in more penalization, further reducing the variance of the model (though raising the bias).

In order to compare the performance of ridge regression against OLS regression and other types of model, I fit the regularized model with the optimal choice of \(\lambda\) to the full `internal_train` dataset. I then compute an RMSE (in terms of the outcome variable \(y\) or RMSLE in terms of the `cnt` variable) by using the trained model to predict outcomes on the `internal_test` dataset.


### Regularization: The Lasso

The final model I train is the lasso. The lasso is similar to ridge regression, in that it is also a way of penalizing coefficients towards zero. Its difference from ridge regression is that for some values of \(\lambda\) the lasso also leads to the coefficients on some features' being reduced all the way to zero. In other words, it performs feature selection, excluding some features entirely from the model. This reduces the complexity of the model, aiding model interpretation, while also lowering model variance.

In the lasso, we seek the set of coefficients \(\hat{b}_{p}\) for parameters \(p\) which minimizes the following cost function:

\[\sum_{i}(y_{i} - b_{0} - b_{g} - b_{t} - b_{w})^2 + \lambda(\sum_{g}\mid b_{g}\mid + \sum_{t}\mid b_{t}\mid + \sum_{w}\mid b_{w}\mid),\]

The only difference between the lasso and ridge cost functions is that instead of penalizing using *squared* coefficient terms - as in ridge - the lasso instead uses the *absolute* values of coefficients in the penalty term.

As for ridge, I use the glmnet package to estimate the lasso. I select the lasso by setting the `alpha` parameter to 1. I also use the same 100 \(\lambda\) values in the range between \(10^{-10}\) and \(10^{10}\). As before, for each \(\lambda\), I use 10-fold cross-validation on the `internal_train` dataset to obtain an MSE, and select the \(\lambda\) one standard error away from the \(\lambda\) that minimises the MSE. This \(\lambda\) results in more penalization, and therefore more features will be removed than at the \(\lambda\) that minimises the MSE.

To compare the models, again I fit the model again using the final choice of \(\lambda\) to the full `internal_train` dataset and calculate the RMSE on the `internal_test` set.


# Results

## Baseline Results

Figure 9 shows the size of the coefficients on each feature in the OLS regression, excluding the intercept. From this, we see that the features with the largest intercepts are those we would expect: the pre-calculated group averages. The `cnt_month_wkday_holiday` feature has a very small coefficient, as might be expected, since it excludes the hour variable. The feature with the largest coefficient is the group averages for hour, month, and day of the week. This was anticipated by Figure 5. 

Other than the group features, the hours of the day remain those features with the most predictive power. The day of the week has very little to add, once the group averages with the hour and month are already included. The estimated intercept is `r format(as.matrix(results_lm$fit$coefficients)[1,], digits = 3)`, which equals the mean of \(\hat{y}_{i}\) and is substantially larger than the nearest coefficient.

Using the fitted model to predict the outcome in the `internal_test` dataset, I obtain an RMSLE of `r format(results_lm$rmsle_out, digit = 4)`.


```{r fig9, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "OLS Coefficient Estimates"}
tibble(Feature = rownames(as.matrix(results_lm$fit$coefficients)),
       Estimate = results_lm$fit$coefficients) %>%
  slice(2:length(Feature)) %>%
  mutate(Feature = reorder(Feature, Estimate, FUN = "identity")) %>%
  ggplot(aes(Feature, Estimate)) + geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(-0.75, 1)) +
  theme_parts + theme(axis.text.x = element_text(angle = 90, hjust = TRUE, vjust = FALSE),
                      aspect.ratio = 2/7)
```

## Ridge Regression

Figure 10 illustrates how the average MSE from 10-fold cross-validation varies for different value of \(\lambda\) for ridge regression. We see that the MSE is minimised when \(\log(\lambda)\) is less than zero, or \(\lambda\) is less than one. The preferred \(\lambda\) parameter is \(\lambda\) = `r results_L2$cv$lambda.1se`, only slightly above zero. Re-estimating the model on the full `internal_train` dataset and using this choice of \(\lambda\), I obtain an RMSLE of `r format(results_L2$rmsle_out, digit = 4)` on the `internal_test` set.

```{r fig10, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Ridge Regression Cross-Validation MSEs for Different Choices of Lambda"}
plot(results_L2$cv)
```


## Lasso Regression

Likewise, Figure 11 illustrates the average MSE for the lasso from 10-fold cross-validation for different value of \(\lambda\). Again we see that the MSE is minimised when \(\log(\lambda)\) is less than zero, or \(\lambda\) is less than one. Across the top of the figure we can also see the number of features the model includes, decreasing rightwards across the figure. The \(\lambda\) I choose to use is \(\lambda\) = `r format(results_L1$cv$lambda.1se, digit = 4)`, again close to zero.

```{r fig11, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Lasso Regression Cross-Validation MSEs for Different Choices of Lambda"}
plot(results_L1$cv)
```

Re-estimating the model on the full `internal_train` dataset, using my choice of \(\lambda\), I obtain an RMSLE of `r format(results_L1$rmsle_out, digit = 4)` on the `internal_test` set. Figure 12 illustrates the coefficient estimates in the final lasso model. We see that the lasso model leaves `r tibble(Estimate = as.matrix(results_L1$fit$beta[, which(results_L1$fit$lambda == results_L1$cv$lambda.1se)])) %>% filter(Estimate != 0) %>% count() %>% pull()` of the original 60 features in the model.


```{r fig12, fig.height = 4.5, fig.width = 7.333333, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Lasso Coefficient Estimates"}
tibble(Feature = rownames(results_L1$fit$beta),
       Estimate = as.matrix(results_L1$fit$beta[, which(results_L1$fit$lambda == results_L1$cv$lambda.1se)])) %>%
  filter(Estimate != 0) %>%
  mutate(Feature = reorder(Feature, Estimate, FUN = "identity")) %>%
  ggplot(aes(Feature, Estimate)) + geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(-0.75, 1)) +
  theme_parts + theme(axis.text.x = element_text(angle = 90, hjust = TRUE, vjust = FALSE),
                      aspect.ratio = 2/7)
```

## Fitting a Final Model

Table \ref{tab:tab3} summarises the results for each model fitted to the full `internal_train` subset of the `training` dataset. Since the ridge regression model has the lowest RMSLE, I select it as my final model. I note however that the benefit of applying regularization to the OLS model appears to be very limited, and in fact the OLS model may perform as well or even better than the ridge regression model when predicting outcomes on an unseen dataset. The reason for this is likely due to the fact that most of the predictive power resides in a couple of the features in the model.

Before testing on the `validation` set, to maximise the precision of the coefficient estimates, I fit the ridge regression to the full `training` dataset, using the optimal \(\lambda\) parameter obtained for that model of \(\lambda\) = `r results_L2$cv$lambda.1se`. Once fitted to the `training` dataset, I obtain a final RMSLE value of `r format(rmse_validation, digit = 4)` on the `validation` set.

```{r tab3, echo = FALSE, message = FALSE, warning = FALSE}
tibble(
  Sample = c("`internal_train`", "`internal_test`"),
  OLS   = c(results_lm$rmsle_in, results_lm$rmsle_out),
  Ridge = c(results_L2$rmsle_in, results_L2$rmsle_out),
  Lasso = c(results_L1$rmsle_in, results_L1$rmsle_out)
) %>%
  mutate(OLS = format(OLS, digits = 4),
         Ridge = format(Ridge, digits = 4),
         Lasso = format(Lasso, digits = 4)) %>%
  knitr::kable("latex", booktabs = T, caption = "RMSLE Results on the Training Set") %>% kable_styling(position = "center", latex_options = c("hold_position"))
```


# Concluding Remarks

In this project, I use data science and machine learning techniques learned on the HarvardX [*Data Science Professional Certificate*](https://www.edx.org/professional-certificate/harvardx-data-science) programme on [edX](https://www.edx.org/) to predict the demand for shared bikes in London. This paper forms my final project for the capstone course.

More specifically, I construct an algorithm to predict the number of new bike journeys in a given hour using hourly data on shared bikes in London from `r format(summary_stats_full$start_date, "%d %B %Y")` to `r format(summary_stats_full$last_date, "%d %B %Y")`. The data comes from Kaggle, the online data science community, which also offers a competition to [forecast shared bike demand]( https://www.kaggle.com/c/bike-sharing-demand) on a scheme in Washington DC.^[See the competition's page for further details (https://www.kaggle.com/c/bike-sharing-demand), last accessed 23 April 2020.]

The demand for shared bikes is measured as a frequency or count variable, which tallies the number of new trips started each hour. This means it cannot take on negative values, and unlike the Gaussian distribution is skewed with a long tail towards higher count values. As a result, it is common to log-transform the count variable and use the Root Mean Squared Logarithmic Error or RMSLE to evaluate the predictive performance of different algorithms.

In this paper, I compare a number of machine learning models for predicting bike demand. First, I train a standard linear model to predict the log-transformed count outcome. Second, I train two penalized linear models - ridge regression and the lasso - to see whether I can improve the prediction. I find that, comparing these algorithms on my `training` data, the ridge regression is the best performing. I therefore use it to predict shared bike usage on my `validation` dataset. I note however that the benefit of using ridge regression over using an OLS model appears to be limited. Overall, I obtain a final RMSLE of `r format(rmse_validation, digit = 4)` on the `validation` set.

There are a number of ways this analysis could be built upon. First, I carry out very limited cleaning of the data and feature selection. According to the plots reported by `R` for the OLS model (not shown in this paper), a number of observations are outliers and/or have high leverage. These points would be worth investigating. Additionally, changing or adding to the group averages included in the models is an obvious way of refining the model, since interacting different features is one potential route to improving prediction. We see that the RMSLE on the `validation` set is substantially higher than those obtained on the `internal_test` subset of the training set, so it would be worth considering what causes the difference and whether it is down to overfitting.

We also see that some features are much more important than others. One idea would be to use different \(\lambda\) for different parameters. For instance, we might not want to regularize the coefficients on the group average features at all, but only those on the other features. This is something that the glmnet package permits.^[See the Glmnet Vignette, p. 14 for further details. (https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf), last accessed 23 April 2020.]

Finally, as noted the improvement of the ridge and lasso models over the standard OLS regression appear relatively limited here. It may be that improving the modelling results can be better achieved not by *restricting* the OLS model, which is what ridge regression and the lasso do, but rather by training a *less constrained*, non-linear model, such as a random forests model.


# References

All links last accessed on 23 April 2020.

- Hastie, T. and J. Qian, *Glmnet Vignette* (https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf). 

- Irizarry, R. A., *Data Analysis and Prediction Algorithms with R*, (https://rafalab.github.io/dsbook/), last updated 1 April 2020.

- James G., D. Witten, T. Hastie, and R. Tibshirani, *An Introduction to Statistical Learning with Applications in R*, Corrected 7th printing (http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf).

- Kaggle, Capital Bikeshare Bike Sharing Demand Competition (https://www.kaggle.com/c/bike-sharing-demand).

- Kaggle, London Bike Share Data Set (https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset).

- Müller, A. C. and S. Guido, *Introduction to Machine Learning with Python*, O'Reilly (https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/ch04.html).

- Tibshirani, R., Data Mining Lecture Slides (https://www.stat.cmu.edu/~ryantibs/datamining/lectures/).

- Transport for London, Cycling Data (https://tfl.gov.uk/modes/cycling/santander-cycles).

- Transport for London, Santander Cycles Docking Station Information (https://tfl.gov.uk/modes/cycling/santander-cycles/find-a-docking-station?intcmp=2321).
